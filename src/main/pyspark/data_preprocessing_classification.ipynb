{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html\n",
    "data_train = '/home/yang/toyData/adult_train.csv'\n",
    "data_test = '/home/yang/toyData/adult_test.csv'\n",
    "path = '/home/yang/derby.log'\n",
    "df.withColumn('c1', when(df.c1.isNotNull(), 1).otherwise(0))\n",
    "  .withColumn('c2', when(df.c2.isNotNull(), 1).otherwise(0))\n",
    "  .withColumn('c3', when(df.c3.isNotNull(), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32561\n",
      "30162\n",
      "+---+----------+-------+----------+-------------+--------------+-------------+--------------+------+-----+------------+------------+--------------+--------------+------+\n",
      "|age| workclass| fnlwgt| education|education-num|marital-status|   occupation|  relationship|  race|  sex|capital-gain|capital-loss|hours-per-week|native-country| class|\n",
      "+---+----------+-------+----------+-------------+--------------+-------------+--------------+------+-----+------------+------------+--------------+--------------+------+\n",
      "| 39| State-gov|77516.0| Bachelors|           13| Never-married| Adm-clerical| Not-in-family| White| Male|      2174.0|         0.0|          40.0| United-States| <=50K|\n",
      "+---+----------+-------+----------+-------------+--------------+-------------+--------------+------+-----+------------+------------+--------------+--------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"age\",IntegerType()),\n",
    "    StructField(\"workclass\", StringType()),\n",
    "    StructField(\"fnlwgt\", DoubleType()),\n",
    "    StructField(\"education\", StringType()),\n",
    "    StructField(\"education-num\",IntegerType()),\n",
    "    StructField(\"marital-status\", StringType()),\n",
    "    StructField(\"occupation\", StringType()),\n",
    "    StructField(\"relationship\", StringType()),\n",
    "    StructField(\"race\", StringType()),\n",
    "    StructField(\"sex\", StringType()),\n",
    "    StructField(\"capital-gain\", DoubleType()),\n",
    "    StructField(\"capital-loss\", DoubleType()),\n",
    "    StructField(\"hours-per-week\", DoubleType()),\n",
    "    StructField(\"native-country\", StringType()),\n",
    "    StructField(\"class\", StringType())\n",
    "])\n",
    "df = spark.read.csv(data_train, header=True, mode=\"DROPMALFORMED\", schema=schema)\n",
    "\n",
    "#na.fill({'age': 50, 'name': 'unknown'}\n",
    "print df.count()\n",
    "df = df.dropna()\n",
    "print df.count()\n",
    "df.show(1)\n",
    "cols = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "categoricalColumns = [\"native-country\",\"sex\",\"race\",\"relationship\",\"occupation\",\"marital-status\",\"education\",\"workclass\"]\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in categoricalColumns:\n",
    "  # Category Indexing with StringIndexer\n",
    "  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n",
    "  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n",
    "  # Add stages.  These are not run here, but will run all at once later on.\n",
    "  stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol = \"class\", outputCol = \"label\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numericCols = [\"fnlwgt\",\"hours-per-week\",\"capital-loss\",\"capital-gain\", \"education-num\",\"age\"]\n",
    "assemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'features', 'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'class']\n",
      "+-----+--------------------+---+----------+-------+----------+-------------+--------------+-------------+--------------+------+-----+------------+------------+--------------+--------------+------+\n",
      "|label|            features|age| workclass| fnlwgt| education|education-num|marital-status|   occupation|  relationship|  race|  sex|capital-gain|capital-loss|hours-per-week|native-country| class|\n",
      "+-----+--------------------+---+----------+-------+----------+-------------+--------------+-------------+--------------+------+-----+------------+------------+--------------+--------------+------+\n",
      "|  0.0|(96,[0,40,41,46,5...| 39| State-gov|77516.0| Bachelors|           13| Never-married| Adm-clerical| Not-in-family| White| Male|      2174.0|         0.0|          40.0| United-States| <=50K|\n",
      "+-----+--------------------+---+----------+-------+----------+-------------+--------------+-------------+--------------+------+-----+------------+------------+--------------+--------------+------+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "# Run the feature transformations.\n",
    "#  - fit() computes feature statistics as needed.\n",
    "#  - transform() actually transforms the features.\n",
    "pipelineModel = pipeline.fit(df)\n",
    "dataset = pipelineModel.transform(df)\n",
    "\n",
    "# Keep relevant columns\n",
    "selectedcols = [\"label\", \"features\"] + cols\n",
    "print selectedcols\n",
    "dataset = dataset.select(selectedcols)\n",
    "print dataset.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'features', 'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'class']\n"
     ]
    }
   ],
   "source": [
    "print dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21160\n",
      "9002\n"
     ]
    }
   ],
   "source": [
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print trainingData.count()\n",
    "print testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "# LogisticRegression.transform() will only use the 'features' column.\n",
    "predictions = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model.save(sc, \"target/tmp/pythonLogisticRegressionWithLBFGSModel\")\n",
    "sameModel = LogisticRegressionModel.load(sc,\n",
    "                                         \"target/tmp/pythonLogisticRegressionWithLBFGSModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BinaryLogisticRegressionTrainingSummary' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-08ccd6d0af5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlrModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'BinaryLogisticRegressionTrainingSummary' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "lrModel.summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+---+---------------+\n",
      "|label|prediction|         probability|age|     occupation|\n",
      "+-----+----------+--------------------+---+---------------+\n",
      "|  0.0|       0.0|[0.66279954980149...| 32| Prof-specialty|\n",
      "|  0.0|       0.0|[0.69474761254951...| 26| Prof-specialty|\n",
      "|  0.0|       0.0|[0.66205880188559...| 31| Prof-specialty|\n",
      "|  0.0|       0.0|[0.54759540543821...| 47| Prof-specialty|\n",
      "+-----+----------+--------------------+---+---------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View model's predictions and probabilities of each prediction class\n",
    "# You can select any columns in the above schema to view as well. For example's sake we will choose age & occupation\n",
    "selected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\n",
    "selected.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.901204168212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'areaUnderROC'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print evaluator.evaluate(predictions)\n",
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: label)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print lr.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictions.select('label', 'prediction')\n",
    "resull_row = prediction.rdd.collect()\n",
    "result_dict = map(lambda c: c.asDict(), resull_row)\n",
    "#result = map(lambda c: c, result_dict)\n",
    "result = sc.parallelize([tuple(d.values()) for d in result_dict])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of True  0.730379071009\n",
      "Precision of False 0.872773179969\n",
      "Recall of True     0.601318681319\n",
      "Recall of False    0.924929389029\n",
      "F-1 Score          0.843145967563\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark-2.1.0-bin-hadoop2.7/python/pyspark/mllib/evaluation.py:262: UserWarning: Deprecated in 2.0.0. Use accuracy.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6222.   505.]\n",
      " [  907.  1368.]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "def printMetrics(predictions_and_labels):\n",
    "    metrics = MulticlassMetrics(predictions_and_labels)\n",
    "    print 'Precision of True ', metrics.precision(1)\n",
    "    print 'Precision of False', metrics.precision(0)\n",
    "    print 'Recall of True    ', metrics.recall(1)\n",
    "    print 'Recall of False   ', metrics.recall(0)\n",
    "    print 'F-1 Score         ', metrics.fMeasure()\n",
    "    print 'Confusion Matrix\\n', metrics.confusionMatrix().toArray()\n",
    "\n",
    "printMetrics(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9007463771548666"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(trainingData)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(testData)\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Intercept:  -1.33815361249\n"
     ]
    }
   ],
   "source": [
    "print 'Model Intercept: ', cvModel.bestModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-0.8735867181722526,), (-1.4715388170278123,), (-0.6258818196179033,), (-0.613905948870067,), (-1.4752616506231644,), (-0.8367092445608016,), (-1.8467401412166615,), (-1.1795819016777898,), (-0.646673098855344,), (-0.8692149014946261,), (-1.1090133115006933,), (-1.8887448632479413,), (-1.2865960236273457,), (-0.39384178799897623,), (-2.116383796353025,), (-1.4209921718510272,), (-1.1281140773015748,), (-0.3473857953741246,), (-0.7071060213873145,), (-2.9203423813399834,), (-0.721663868594423,), (-0.8126116326269156,), (-1.1968075526279958,), (-0.6863103410389289,), (-1.0589023976670786,), (-1.4904023523235366,), (-0.47961952122367724,), (-0.12895622389256026,), (-1.5529272078183434,), (-0.5268365742057032,), (-1.0524341838484204,), (-1.4210045644120506,), (-0.8701369403870748,), (-0.5396424866221428,), (-1.3563644337503775,), (-0.48091363482437494,), (-2.4798542614764223,), (-0.8248301683596001,), (-1.3092246838212545,), (-1.5920813357464134,), (0.4106521682256246,), (-0.5478916371544414,), (-0.651469824555358,), (-0.5443048304342257,), (-1.17572247783444,), (0.36249296327192077,), (-0.20822830619602406,), (-1.147740825940067,), (-0.639435319804967,), (1.2062297455358664,), (0.27574068446861816,), (-0.15479549443567517,), (0.4955723897928911,), (-0.25910582242969243,), (0.06706116460236368,), (-1.1026715087033692,), (-0.4898080516265817,), (-0.37530160359447556,), (-1.0143117276255877,), (-0.9564491407838763,), (0.29046254977930386,), (0.32514484552203543,), (-1.4509685211224133,), (0.5043993234242312,), (-0.9697645456167924,), (-0.46291247337122704,), (-0.5685422635014791,), (-0.34055127357896514,), (-0.5730692383262538,), (-0.5644846741611302,), (-0.23390733109183312,), (0.5713503779335817,), (0.8533197461084551,), (-0.06238748009008853,), (-1.1363167456461316,), (-0.04772869202969766,), (-1.138602161212096,), (-1.77839125838476,), (1.4447686519487206,), (-1.541348975377488,), (-1.1171327568440896,), (1.418515295680219,), (-1.4295206357323456,), (-1.5011700708835551,), (-0.30951896998393463,), (-0.6847862011644315,), (-0.481419038616404,), (-0.6876103530655328,), (-0.10531196763287233,), (0.19253254535576247,), (4.7030322557149516e-07,), (0.023731830451809925,), (0.00045675946546261636,), (0.00012974004057844118,), (-0.026342588019338514,), (0.0183989364876959,)]\n",
      "+-------------------+\n",
      "|     Feature Weight|\n",
      "+-------------------+\n",
      "|-0.8735867181722526|\n",
      "|-1.4715388170278123|\n",
      "|-0.6258818196179033|\n",
      "| -0.613905948870067|\n",
      "|-1.4752616506231644|\n",
      "|-0.8367092445608016|\n",
      "|-1.8467401412166615|\n",
      "|-1.1795819016777898|\n",
      "| -0.646673098855344|\n",
      "|-0.8692149014946261|\n",
      "+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = cvModel.bestModel.coefficients\n",
    "# on Spark 2.X weights are available as ceofficients\n",
    "# weights = cvModel.bestModel.coefficients\n",
    "weights = map(lambda w: (float(w),), weights)  # convert numpy type to float, and to tuple\n",
    "print weights\n",
    "weightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\n",
    "weightsDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create an initial RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "\n",
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "predictions = rfModel.transform(testData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8867171656944173"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
